\documentclass[10pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{subcaption}

% Reduce section spacing
\titlespacing*{\section}{0pt}{6pt}{4pt}
\titlespacing*{\subsection}{0pt}{6pt}{4pt}
\setlength{\parskip}{2pt}

% Compact lists
\setlist{nosep}

% Title formatting
\title{\vspace{-2cm}\textbf{Assignment 1 - NLP using DL techniques (83374)}}
\author{Dana Gibor (322274234) \and Ido Sar Shalom (212410146) \and Natalya Sigal (306688466)}
\date{\vspace{-1cm}}

\begin{document}

\maketitle

\section{Introduction}
This report details the development of deep learning models for classifying text into six emotion categories: \textit{Sadness, Joy, Love, Anger, Fear,} and \textit{Surprise}. We compared two RNN architectures: \textbf{Gated Recurrent Unit (GRU)} and \textbf{Bidirectional Long Short-Term Memory (BiLSTM)}. The project involved extensive preprocessing, distinct embedding strategies, and systematic hyperparameter tuning.

\section{Data Preparation}

\subsection{Preprocessing \& Embeddings}
A robust pipeline cleaned the data by removing URLs, special characters, and numbers, normalizing to lowercase, and removing stopwords. Duplicates were removed from training data only. We employed distinct pre-trained embeddings:
\begin{itemize}
    \item \textbf{Word2Vec (Google News 300d) for GRU:} Captures local context, matching GRU's efficiency with local patterns.
    \item \textbf{GloVe (Twitter 200d) for BiLSTM:} Leverages global co-occurrence statistics, complementing LSTM's long-term dependency handling.
\end{itemize}

\section{Model Architectures \& Experiments}
Both models use a sequential architecture: Embedding $\rightarrow$ Bidirectional RNN $\rightarrow$ BatchNorm $\rightarrow$ Dropout $\rightarrow$ Dense (ReLU) $\rightarrow$ Dropout $\rightarrow$ Output.

\subsection{Hyperparameter Tuning}
We systematically tested Learning Rate (0.0001--0.01), Batch Size (16--128), Hidden Units (64--256), and Dropout (0.2--0.6) to maximize validation accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{GRU_hyperparameter_tuning_results.jpg}
    \caption{Impact of hyperparameters on GRU model performance.}
    \label{fig:gru_tuning}
\end{figure}

\subsection{Optimal Configurations}
\textbf{GRU (Word2Vec):} The optimal configuration was \textbf{LR: 0.005, Batch: 16, Units: 96, Dropout: 0.4} with \textbf{Adam} optimizer and Frozen embeddings. This yielded the highest accuracy (0.9345) and lowest loss. Frozen embeddings slightly outperformed fine-tuned ones (0.9340 vs 0.9210).

\textbf{BiLSTM (GloVe):} The best setup was \textbf{LR: 0.001, Batch: 32, Units: 192, Dropout: 0.3} with \textbf{Adam}. It required lower learning rates and significantly more units than GRU to reach peak performance (0.9250).

\section{Results \& Convergence}
Both models exceeded the 75\% target accuracy, with GRU converging faster ($\sim$10 epochs) than BiLSTM ($\sim$15 epochs).

\begin{table}[H]
\centering
\small
\caption{Performance Comparison}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{GRU (Word2Vec)} & \textbf{BiLSTM (GloVe)} \\
\midrule
Validation Accuracy & \textbf{92.80\%} & 91.65\% \\
Validation Loss & \textbf{0.1344} & 0.1998 \\
Optimal Units & 96 & 192 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{GRU_training.jpg}
        \caption{GRU Training History}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{LSTM_training.jpg}
        \caption{BiLSTM Training History}
    \end{subfigure}
    \caption{Training Accuracy and Loss over epochs.}
    \label{fig:training_history}
\end{figure}

\section{Conclusion}
The \textbf{GRU model with Word2Vec} is the optimal configuration.
\begin{enumerate}
    \item \textbf{Performance:} Higher accuracy (92.80\% vs 91.65\%) and lower loss.
    \item \textbf{Efficiency:} Required fewer units (96 vs 192), leading to a lighter, faster model.
    \item \textbf{Embeddings:} 300d Word2Vec embeddings likely provided richer features than 200d GloVe.
\end{enumerate}
Thus, the simpler GRU architecture paired with high-dimensional embeddings outperforms the more complex BiLSTM for this task.

\end{document}


