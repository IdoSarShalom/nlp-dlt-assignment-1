\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=0.85in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage{enumitem}

% Slight reduction in section spacing
\titlespacing*{\section}{0pt}{10pt}{5pt}
\titlespacing*{\subsection}{0pt}{8pt}{4pt}
\setlist{itemsep=2pt, topsep=2pt}

% Title formatting
\title{\vspace{-1cm}\textbf{Assignment 1 - NLP using DL techniques (83374)}}
\author{Dana Gibor (322274234) \and Ido Sar Shalom (212410146) \and Natalya Sigal (306688466)}
\date{}

\begin{document}

\maketitle

\section{Introduction}
This report details the development and evaluation of deep learning models for classifying text into six emotion categories: \textit{Sadness, Joy, Love, Anger, Fear,} and \textit{Surprise}. We implemented and compared two RNN architectures: \textbf{Gated Recurrent Unit (GRU)} and \textbf{Bidirectional Long Short-Term Memory (BiLSTM)}. The project involved extensive data preprocessing, distinct embedding strategies, systematic hyperparameter tuning, and a comparative analysis to identify the optimal approach.

\section{Exploratory Data Analysis}
Before proceeding to data preparation, we conducted an analysis of the class balance within the dataset.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{class_distribution_placeholder.jpg}
    \caption{Distribution of Emotion Classes.}
    \label{fig:class_dist}
\end{figure}

\noindent The distribution, as illustrated in Figure \ref{fig:class_dist}, reveals a significant class imbalance. The dataset is dominated by \textit{Joy} (33.5\%) and \textit{Sadness} (29.2\%), which together constitute over 60\% of the samples. In contrast, classes such as \textit{Surprise} (3.6\%) and \textit{Love} (8.2\%) are heavily underrepresented. This disparity suggests a potential bias where the model may favor majority classes.

\section{Data Preparation}

\subsection{Data Preprocessing}
A robust pipeline was implemented to clean and prepare the data. Key steps included:
\begin{itemize}
    \item \textbf{Cleaning \& Normalization:} Removing URLs, special characters, numbers, and converting text to lowercase.
    \item \textbf{Filtering:} Removing standard stopwords and non-alphanumeric characters.
    \item \textbf{Duplicate Handling:} Strictly removed from training to prevent overfitting, but retained in validation/test sets.
\end{itemize}

\subsection{Embedding Strategies}
Distinct pre-trained embeddings were chosen to complement each network's strengths:
\begin{itemize}
    \item \textbf{Word2Vec (Google News 300d) with GRU:} A predictive model capturing local context patterns. This matches the GRU's efficiency with local temporal patterns.
    \item \textbf{GloVe (Twitter 200d) with BiLSTM:} A count-based model leveraging global co-occurrence statistics. This complements the LSTM's ability to handle long-term dependencies and aligns with the informal dataset nature.
\end{itemize}

\section{Model Architectures \& Experiments}
Both models utilized a sequential architecture: Embedding $\rightarrow$ Bidirectional RNN $\rightarrow$ BatchNorm $\rightarrow$ Dropout $\rightarrow$ Dense $\rightarrow$ Output.

\subsection{Hyperparameter Tuning Methodology}
We adopted a systematic approach, running controlled experiments for each hyperparameter:
\begin{itemize}
    \item \textbf{Learning Rate:} Tested $0.0001-0.01$ to find the convergence "sweet spot".
    \item \textbf{Batch Size:} Tested $16, 32, 64, 128$. Smaller batches provided better regularization.
    \item \textbf{Hidden Units:} Tested $64-256$ to determine the minimum capacity needed to avoid overfitting.
    \item \textbf{Dropout Rate:} Tested $0.2-0.6$ to prevent memorization.
\end{itemize}

\noindent To manage computational constraints effectively, we employed a pragmatic \textbf{One-Factor-at-a-Time (OFAT)} tuning strategy instead of an exhaustive Grid Search. This heuristic approach involved establishing a baseline configuration and systematically varying one hyperparameter (e.g., Learning Rate) while holding all others constant.

\noindent The final configurations were selected based strictly on maximizing \textbf{Validation Accuracy} while minimizing \textbf{Validation Loss}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{GRU_hyperparameter_tuning_results.jpg}
    \caption{Impact of various hyperparameters on GRU model performance.}
    \label{fig:gru_tuning}
\end{figure}

\subsection{Optimal Configurations}
Systematic tuning yielded the following optimal configurations:

\noindent
\textbf{GRU Architecture (Word2Vec):}
\begin{itemize}
    \item \textbf{Config:} LR: 0.005, Batch: 16, Units: 96, Dropout: 0.4, Optimizer: Adam, Embeddings: Frozen.
    \item \textbf{Result:} Highest accuracy (0.9345). Frozen embeddings prevented overfitting.
\end{itemize}

\noindent
\textbf{BiLSTM Architecture (GloVe):}
\begin{itemize}
    \item \textbf{Config:} LR: 0.001, Batch: 32, Units: 192, Dropout: 0.3, Optimizer: Adam.
    \item \textbf{Result:} Required lower learning rate and significantly more capacity (192 vs 96 units) than GRU.
\end{itemize}

\section{Results}
Both models exceeded the 75\% target accuracy. Early stopping (patience=5) prevented overfitting.

\begin{table}[H]
\centering
\caption{Performance Comparison Summary}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{GRU Model} & \textbf{BiLSTM Model} \\
\midrule
\textbf{Embedding} & Word2Vec (300d) & GloVe (200d) \\
\textbf{Validation Accuracy} & \textbf{92.80\%} & 91.65\% \\
\textbf{Validation Loss} & \textbf{0.1344} & 0.1998 \\
\textbf{Optimal Units} & 96 & 192 \\
\textbf{Converged Epochs} & $\sim$10 & $\sim$15 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Error Analysis}
To gain deeper insights into model performance, we examined the confusion matrix of the GRU network.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{confusion_matrix_placeholder.jpg}
    \caption{Confusion Matrix for the GRU model.}
    \label{fig:conf_matrix}
\end{figure}

\noindent The analysis reveals that while the model generally performs well, there are notable shared misclassifications. In particular, we observe confusion between similar emotions like \textit{Fear} and \textit{Sadness} or \textit{Love} and \textit{Joy}. This overlap is likely due to the shared vocabulary and semantic proximity of these emotions, a pattern that was observed to affect the BiLSTM model as well.

\newpage

\subsection{Convergence Analysis}
The GRU model converged faster (epoch 10-11) with stable loss reduction. The BiLSTM model required more epochs and higher capacity (units) to reach comparable performance.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{GRU_training.jpg}
        \caption{GRU Training History}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{LSTM_training.jpg}
        \caption{BiLSTM Training History}
    \end{subfigure}
    \caption{Training Accuracy and Loss over epochs.}
    \label{fig:training_history}
\end{figure}

\section{Conclusion}
Based on the comparative analysis of the validation set, the \textbf{GRU model with Word2Vec embeddings} appears to be the optimal configuration for this emotion classification task.

\noindent While these results strongly favor the GRU architecture, it is important to note that performance on the unseen test set may vary. However, given the consistent superiority in validation metrics and computational efficiency, the GRU model is our recommended candidate for deployment.

\end{document}