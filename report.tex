\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=0.85in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage{enumitem}

% Slight reduction in section spacing
\titlespacing*{\section}{0pt}{10pt}{5pt}
\titlespacing*{\subsection}{0pt}{8pt}{4pt}
\setlist{itemsep=2pt, topsep=2pt}

% Title formatting
\title{\vspace{-1cm}\textbf{Assignment 1 - NLP using DL techniques (83374)}}
\author{Dana Gibor (322274234) \and Ido Sar Shalom (212410146) \and Natalya Sigal (306688466)}
\date{}

\begin{document}

\maketitle

\section{Introduction}
This report details the development and evaluation of deep learning models for classifying text into six emotion categories: \textit{Sadness, Joy, Love, Anger, Fear,} and \textit{Surprise}. We implemented and compared two RNN architectures: \textbf{Gated Recurrent Unit (GRU)} and \textbf{Bidirectional Long Short-Term Memory (BiLSTM)}. The project involved extensive data preprocessing, distinct embedding strategies, systematic hyperparameter tuning, and a comparative analysis to identify the optimal approach.

\section{Data Preparation}

\subsection{Data Preprocessing}
A robust pipeline was implemented to clean and prepare the data. Key steps included:
\begin{itemize}
    \item \textbf{Cleaning \& Normalization:} Removing URLs, special characters, numbers, and converting text to lowercase.
    \item \textbf{Filtering:} Removing standard stopwords and non-alphanumeric characters.
    \item \textbf{Duplicate Handling:} Strictly removed from training to prevent overfitting, but retained in validation/test sets.
\end{itemize}

\subsection{Embedding Strategies}
Distinct pre-trained embeddings were chosen to complement each network's strengths:
\begin{itemize}
    \item \textbf{Word2Vec (Google News 300d) with GRU:} A predictive model capturing local context patterns. This matches the GRU's efficiency with local temporal patterns.
    \item \textbf{GloVe (Twitter 200d) with BiLSTM:} A count-based model leveraging global co-occurrence statistics. This complements the LSTM's ability to handle long-term dependencies and aligns with the informal dataset nature.
\end{itemize}

\section{Model Architectures \& Experiments}
Both models utilized a sequential architecture: Embedding $\rightarrow$ Bidirectional RNN $\rightarrow$ BatchNorm $\rightarrow$ Dropout $\rightarrow$ Dense $\rightarrow$ Output.

\subsection{Hyperparameter Tuning Methodology}
We adopted a systematic approach, running controlled experiments for each hyperparameter:
\begin{itemize}
    \item \textbf{Learning Rate:} Tested $0.0001-0.01$ to find the convergence "sweet spot".
    \item \textbf{Batch Size:} Tested $16, 32, 64, 128$. Smaller batches provided better regularization.
    \item \textbf{Hidden Units:} Tested $64-256$ to determine the minimum capacity needed to avoid overfitting.
    \item \textbf{Dropout Rate:} Tested $0.2-0.6$ to prevent memorization.
\end{itemize}

\noindent The final configurations were selected based strictly on maximizing \textbf{Validation Accuracy} while minimizing \textbf{Validation Loss}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{GRU_hyperparameter_tuning_results.jpg}
    \caption{Impact of various hyperparameters on GRU model performance.}
    \label{fig:gru_tuning}
\end{figure}

\subsection{Optimal Configurations}
Systematic tuning yielded the following optimal configurations:

\noindent
\textbf{GRU Architecture (Word2Vec):}
\begin{itemize}
    \item \textbf{Config:} LR: 0.005, Batch: 16, Units: 96, Dropout: 0.4, Optimizer: Adam, Embeddings: Frozen.
    \item \textbf{Result:} Highest accuracy (0.9345). Frozen embeddings prevented overfitting.
\end{itemize}

\noindent
\textbf{BiLSTM Architecture (GloVe):}
\begin{itemize}
    \item \textbf{Config:} LR: 0.001, Batch: 32, Units: 192, Dropout: 0.3, Optimizer: Adam.
    \item \textbf{Result:} Required lower learning rate and significantly more capacity (192 vs 96 units) than GRU.
\end{itemize}

\section{Results \& Convergence Analysis}
Both models exceeded the 75\% target accuracy. Early stopping (patience=5) prevented overfitting.

\begin{table}[H]
\centering
\caption{Performance Comparison Summary}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{GRU Model} & \textbf{BiLSTM Model} \\
\midrule
\textbf{Embedding} & Word2Vec (300d) & GloVe (200d) \\
\textbf{Validation Accuracy} & \textbf{92.80\%} & 91.65\% \\
\textbf{Validation Loss} & \textbf{0.1344} & 0.1998 \\
\textbf{Optimal Units} & 96 & 192 \\
\textbf{Converged Epochs} & $\sim$10 & $\sim$15 \\
\bottomrule
\end{tabular}
\end{table}

\newpage

\subsection{Convergence Analysis}
The GRU model converged faster (epoch 10-11) with stable loss reduction. The BiLSTM model required more epochs and higher capacity (units) to reach comparable performance.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{GRU_training.jpg}
        \caption{GRU Training History}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{LSTM_training.jpg}
        \caption{BiLSTM Training History}
    \end{subfigure}
    \caption{Training Accuracy and Loss over epochs.}
    \label{fig:training_history}
\end{figure}

\section{Conclusion}
Based on the comparative analysis of the validation set, the \textbf{GRU model with Word2Vec embeddings} appears to be the optimal configuration for this emotion classification task.

\noindent While these results strongly favor the GRU architecture, it is important to note that performance on the unseen test set may vary. However, given the consistent superiority in validation metrics and computational efficiency, the GRU model is our recommended candidate for deployment.

\end{document}


