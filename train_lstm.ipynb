{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî• BiLSTM Model Training: Emotion Classification\n",
        "\n",
        "This notebook trains a Bidirectional LSTM model on preprocessed emotion data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed training and validation data\n",
        "train_df = pd.read_pickle('./data/train_preprocessed.pkl')\n",
        "val_df = pd.read_pickle('./data/validation_preprocessed.pkl')\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Validation data shape: {val_df.shape}\")\n",
        "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(train_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Prepare Data\n",
        "\n",
        "Split the data into features (X) and labels (y).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X_train = train_df['Text']\n",
        "y_train = train_df['Label']\n",
        "X_val = val_df['Text']\n",
        "y_val = val_df['Label']\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "print(y_train.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî† Tokenization\n",
        "\n",
        "Convert text to sequences of integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=60000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Find maximum sequence length\n",
        "maxlen = max(len(tokens) for tokens in X_train_sequences)\n",
        "print(f\"Maximum sequence length: {maxlen}\")\n",
        "\n",
        "# Pad sequences\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post')\n",
        "X_val_padded = pad_sequences(X_val_sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "print(f\"\\nX_train_padded shape: {X_train_padded.shape}\")\n",
        "print(f\"X_val_padded shape: {X_val_padded.shape}\")\n",
        "\n",
        "# Calculate input size for embedding layer\n",
        "input_size = np.max(X_train_padded) + 1\n",
        "print(f\"Vocabulary size (input_size): {input_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Build BiLSTM Model\n",
        "\n",
        "**Model Architecture:**\n",
        "1. **Embedding Layer**: Converts word indices to dense vectors\n",
        "2. **Bidirectional LSTM**: Processes sequences in both directions (forward and backward)\n",
        "3. **Batch Normalization**: Normalizes activations for stable training\n",
        "4. **Dropout**: Prevents overfitting (50% dropout rate)\n",
        "5. **Dense Layer**: Fully connected layer with ReLU activation\n",
        "6. **Output Layer**: 6 units with softmax for emotion classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the BiLSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(input_dim=input_size, output_dim=100, input_shape=(maxlen,)))\n",
        "\n",
        "# Bidirectional LSTM layer with 128 units\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "# Batch normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer with ReLU activation\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer with 6 units (6 emotions) and softmax activation\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Train the Model\n",
        "\n",
        "Train with early stopping to prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_padded, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_padded, y_val),\n",
        "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualize Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the epoch with the highest validation accuracy\n",
        "best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\n",
        "\n",
        "# Create a subplot with 1 row and 2 columns\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "axs[0].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "axs[0].scatter(best_epoch - 1, history.history['val_accuracy'][best_epoch - 1], \n",
        "               color='green', label=f'Best Epoch: {best_epoch}')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_title('Training and Validation Accuracy')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot training and validation loss\n",
        "axs[1].plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "axs[1].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "axs[1].scatter(best_epoch - 1, history.history['val_loss'][best_epoch - 1], \n",
        "               color='green', label=f'Best Epoch: {best_epoch}')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].set_title('Training and Validation Loss')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Evaluate Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "val_loss, val_accuracy = model.evaluate(X_val_padded, y_val)\n",
        "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "y_val_pred = model.predict(X_val_padded)\n",
        "y_val_pred = np.argmax(y_val_pred, axis=1)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
        "            xticklabels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'],\n",
        "            yticklabels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('BiLSTM Model - Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print classification report\n",
        "emotion_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_val_pred, target_names=emotion_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Model and Tokenizer\n",
        "\n",
        "Save the trained model and tokenizer for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('./data/lstm_model.keras')\n",
        "print(\"‚úÖ Model saved to: ./data/lstm_model.keras\")\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('./data/lstm_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"‚úÖ Tokenizer saved to: ./data/lstm_tokenizer.pkl\")\n",
        "\n",
        "# Save training metadata\n",
        "metadata = {\n",
        "    'maxlen': maxlen,\n",
        "    'input_size': input_size,\n",
        "    'label_mapping': label_mapping,\n",
        "    'val_accuracy': val_accuracy,\n",
        "    'val_loss': val_loss,\n",
        "    'best_epoch': best_epoch\n",
        "}\n",
        "\n",
        "with open('./data/lstm_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "print(\"‚úÖ Metadata saved to: ./data/lstm_metadata.pkl\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"BiLSTM MODEL TRAINING COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
