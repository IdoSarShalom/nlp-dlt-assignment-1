{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA) - Emotions Dataset\n",
        "\n",
        "This notebook provides comprehensive exploratory data analysis of the emotions training dataset.\n",
        "\n",
        "## Analysis Sections:\n",
        "1. **Dataset Overview** - Basic statistics and structure\n",
        "2. **Class Distribution** - Emotion label analysis\n",
        "3. **Text Length Analysis** - Character and word count distributions\n",
        "4. **Vocabulary Analysis** - Unique words and vocabulary richness\n",
        "5. **Word Frequency Analysis** - Most common words per emotion\n",
        "6. **Word Clouds** - Visual representation of emotion-specific vocabulary\n",
        "7. **N-gram Analysis** - Common phrases (bigrams, trigrams)\n",
        "8. **Text Complexity Metrics** - Linguistic diversity measures\n",
        "9. **Sample Text Inspection** - Example texts from each emotion\n",
        "10. **Statistical Summary** - Key insights and recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "df = pd.read_csv('./data/train.csv')\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"\\nüìä First 10 rows of the dataset:\")\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"\\nüìã Dataset Information:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"\\nüìà Statistical Summary:\")\n",
        "df.describe(include='all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\nüîç Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing)\n",
        "print(f\"\\nTotal missing values: {missing.sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"\\nüîç Duplicate rows: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"\\nSample duplicate entries:\")\n",
        "    display(df[df.duplicated(keep=False)].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Emotion Label Mapping & Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define emotion label mapping\n",
        "emotion_mapping = {\n",
        "    0: 'Sadness',\n",
        "    1: 'Joy',\n",
        "    2: 'Love',\n",
        "    3: 'Anger',\n",
        "    4: 'Fear',\n",
        "    5: 'Surprise'\n",
        "}\n",
        "\n",
        "# Create emotion name column for better visualization\n",
        "df['emotion_name'] = df['label'].map(emotion_mapping)\n",
        "\n",
        "print(\"\\nüè∑Ô∏è Emotion Label Mapping:\")\n",
        "for label, emotion in emotion_mapping.items():\n",
        "    print(f\"  {label} ‚Üí {emotion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "print(\"\\nüìä Class Distribution:\")\n",
        "class_counts = df['emotion_name'].value_counts().sort_index()\n",
        "class_percentages = (class_counts / len(df) * 100).round(2)\n",
        "\n",
        "distribution_df = pd.DataFrame({\n",
        "    'Emotion': class_counts.index,\n",
        "    'Count': class_counts.values,\n",
        "    'Percentage': class_percentages.values\n",
        "})\n",
        "\n",
        "display(distribution_df)\n",
        "\n",
        "# Check for class imbalance\n",
        "max_ratio = class_counts.max() / class_counts.min()\n",
        "print(f\"\\n‚öñÔ∏è Class imbalance ratio (max/min): {max_ratio:.2f}\")\n",
        "if max_ratio > 3:\n",
        "    print(\"‚ö†Ô∏è Warning: Significant class imbalance detected!\")\n",
        "else:\n",
        "    print(\"‚úÖ Classes are relatively balanced\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Pie chart\n",
        "colors = sns.color_palette('husl', len(class_counts))\n",
        "axes[0].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', \n",
        "            startangle=140, colors=colors)\n",
        "axes[0].set_title('Emotion Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Bar chart\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, ax=axes[1], palette='husl')\n",
        "axes[1].set_title('Emotion Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Emotion', fontsize=12)\n",
        "axes[1].set_ylabel('Count', fontsize=12)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add count labels on bars\n",
        "for i, v in enumerate(class_counts.values):\n",
        "    axes[1].text(i, v + 100, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Length Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate text statistics\n",
        "df['text_length'] = df['text'].str.len()\n",
        "df['word_count'] = df['text'].str.split().str.len()\n",
        "df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
        "df['unique_words'] = df['text'].apply(lambda x: len(set(x.split())))\n",
        "df['unique_word_ratio'] = df['unique_words'] / df['word_count']\n",
        "\n",
        "print(\"‚úÖ Text features calculated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall text length statistics\n",
        "print(\"\\nüìè Overall Text Length Statistics:\")\n",
        "print(f\"  Character count - Mean: {df['text_length'].mean():.2f}, Median: {df['text_length'].median():.2f}\")\n",
        "print(f\"  Character count - Min: {df['text_length'].min()}, Max: {df['text_length'].max()}\")\n",
        "print(f\"  Word count - Mean: {df['word_count'].mean():.2f}, Median: {df['word_count'].median():.2f}\")\n",
        "print(f\"  Word count - Min: {df['word_count'].min()}, Max: {df['word_count'].max()}\")\n",
        "print(f\"  Average word length - Mean: {df['avg_word_length'].mean():.2f}\")\n",
        "print(f\"  Unique word ratio - Mean: {df['unique_word_ratio'].mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text length statistics by emotion\n",
        "print(\"\\nüìä Text Length Statistics by Emotion:\")\n",
        "length_stats = df.groupby('emotion_name')[['text_length', 'word_count', 'avg_word_length']].agg(['mean', 'median', 'std'])\n",
        "display(length_stats.round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize text length distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Character count distribution\n",
        "axes[0, 0].hist(df['text_length'], bins=50, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('Character Count Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Number of Characters')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df['text_length'].mean(), color='red', linestyle='--', label=f\"Mean: {df['text_length'].mean():.1f}\")\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Word count distribution\n",
        "axes[0, 1].hist(df['word_count'], bins=50, color='lightcoral', edgecolor='black')\n",
        "axes[0, 1].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Number of Words')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].axvline(df['word_count'].mean(), color='red', linestyle='--', label=f\"Mean: {df['word_count'].mean():.1f}\")\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Word count by emotion (boxplot)\n",
        "df_sorted = df.sort_values('emotion_name')\n",
        "sns.boxplot(data=df_sorted, x='emotion_name', y='word_count', ax=axes[1, 0], palette='Set2')\n",
        "axes[1, 0].set_title('Word Count Distribution by Emotion', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Emotion')\n",
        "axes[1, 0].set_ylabel('Word Count')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Character count by emotion (violin plot)\n",
        "sns.violinplot(data=df_sorted, x='emotion_name', y='text_length', ax=axes[1, 1], palette='Set3')\n",
        "axes[1, 1].set_title('Character Count Distribution by Emotion', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Emotion')\n",
        "axes[1, 1].set_ylabel('Character Count')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate vocabulary statistics\n",
        "all_words = ' '.join(df['text']).lower().split()\n",
        "total_words = len(all_words)\n",
        "unique_words = len(set(all_words))\n",
        "vocab_richness = unique_words / total_words\n",
        "\n",
        "print(\"\\nüìö Vocabulary Statistics:\")\n",
        "print(f\"  Total words: {total_words:,}\")\n",
        "print(f\"  Unique words: {unique_words:,}\")\n",
        "print(f\"  Vocabulary richness: {vocab_richness:.4f}\")\n",
        "\n",
        "# Most common words overall\n",
        "word_freq = Counter(all_words)\n",
        "most_common = word_freq.most_common(20)\n",
        "\n",
        "print(\"\\nüî§ Top 20 Most Common Words:\")\n",
        "for i, (word, count) in enumerate(most_common, 1):\n",
        "    print(f\"  {i:2d}. {word:15s} - {count:6,} times\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize most common words\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Bar chart of top 20 words\n",
        "words, counts = zip(*most_common)\n",
        "axes[0].barh(range(len(words)), counts, color='steelblue')\n",
        "axes[0].set_yticks(range(len(words)))\n",
        "axes[0].set_yticklabels(words)\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].set_title('Top 20 Most Common Words', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Frequency')\n",
        "\n",
        "# Word frequency distribution (log scale)\n",
        "freq_values = sorted(word_freq.values(), reverse=True)\n",
        "axes[1].plot(freq_values[:1000], color='darkgreen', linewidth=2)\n",
        "axes[1].set_title('Word Frequency Distribution (Top 1000 words)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Rank')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vocabulary size by emotion\n",
        "print(\"\\nüìä Vocabulary Statistics by Emotion:\")\n",
        "vocab_by_emotion = {}\n",
        "\n",
        "for emotion in emotion_mapping.values():\n",
        "    emotion_texts = df[df['emotion_name'] == emotion]['text']\n",
        "    emotion_words = ' '.join(emotion_texts).lower().split()\n",
        "    vocab_by_emotion[emotion] = {\n",
        "        'total_words': len(emotion_words),\n",
        "        'unique_words': len(set(emotion_words)),\n",
        "        'vocab_richness': len(set(emotion_words)) / len(emotion_words)\n",
        "    }\n",
        "\n",
        "vocab_df = pd.DataFrame(vocab_by_emotion).T\n",
        "display(vocab_df.style.format({\n",
        "    'total_words': '{:,.0f}',\n",
        "    'unique_words': '{:,.0f}',\n",
        "    'vocab_richness': '{:.4f}'\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Most Common Words by Emotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top words for each emotion\n",
        "print(\"\\nüéØ Top 10 Most Common Words by Emotion:\\n\")\n",
        "\n",
        "for emotion in emotion_mapping.values():\n",
        "    emotion_texts = df[df['emotion_name'] == emotion]['text']\n",
        "    emotion_words = ' '.join(emotion_texts).lower().split()\n",
        "    emotion_word_freq = Counter(emotion_words)\n",
        "    top_words = emotion_word_freq.most_common(10)\n",
        "    \n",
        "    print(f\"\\n{emotion}:\")\n",
        "    for i, (word, count) in enumerate(top_words, 1):\n",
        "        print(f\"  {i:2d}. {word:15s} - {count:5,} times\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top words by emotion\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, emotion in enumerate(emotion_mapping.values()):\n",
        "    emotion_texts = df[df['emotion_name'] == emotion]['text']\n",
        "    emotion_words = ' '.join(emotion_texts).lower().split()\n",
        "    emotion_word_freq = Counter(emotion_words)\n",
        "    top_words = emotion_word_freq.most_common(10)\n",
        "    \n",
        "    words, counts = zip(*top_words)\n",
        "    axes[idx].barh(range(len(words)), counts, color=sns.color_palette('husl', 6)[idx])\n",
        "    axes[idx].set_yticks(range(len(words)))\n",
        "    axes[idx].set_yticklabels(words)\n",
        "    axes[idx].invert_yaxis()\n",
        "    axes[idx].set_title(f'{emotion}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Frequency', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Top 10 Words by Emotion', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Word Clouds by Emotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate word clouds for each emotion\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "colors = ['Blues', 'Greens', 'Reds', 'Oranges', 'Purples', 'YlOrBr']\n",
        "\n",
        "for idx, emotion in enumerate(emotion_mapping.values()):\n",
        "    emotion_texts = df[df['emotion_name'] == emotion]['text']\n",
        "    combined_text = ' '.join(emotion_texts)\n",
        "    \n",
        "    wordcloud = WordCloud(\n",
        "        width=800, \n",
        "        height=400, \n",
        "        background_color='white',\n",
        "        colormap=colors[idx],\n",
        "        max_words=100\n",
        "    ).generate(combined_text)\n",
        "    \n",
        "    axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
        "    axes[idx].set_title(f'{emotion} Word Cloud', fontsize=14, fontweight='bold')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. N-gram Analysis (Bigrams & Trigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get n-grams\n",
        "def get_ngrams(text_series, n=2, top_k=15):\n",
        "    \"\"\"Extract top k n-grams from text series\"\"\"\n",
        "    all_ngrams = []\n",
        "    for text in text_series:\n",
        "        tokens = text.lower().split()\n",
        "        text_ngrams = list(ngrams(tokens, n))\n",
        "        all_ngrams.extend(text_ngrams)\n",
        "    \n",
        "    ngram_freq = Counter(all_ngrams)\n",
        "    return ngram_freq.most_common(top_k)\n",
        "\n",
        "# Get overall bigrams and trigrams\n",
        "print(\"\\nüî§ Top 15 Bigrams (2-word phrases):\\n\")\n",
        "bigrams = get_ngrams(df['text'], n=2, top_k=15)\n",
        "for i, (bigram, count) in enumerate(bigrams, 1):\n",
        "    print(f\"  {i:2d}. {' '.join(bigram):30s} - {count:5,} times\")\n",
        "\n",
        "print(\"\\nüî§ Top 15 Trigrams (3-word phrases):\\n\")\n",
        "trigrams = get_ngrams(df['text'], n=3, top_k=15)\n",
        "for i, (trigram, count) in enumerate(trigrams, 1):\n",
        "    print(f\"  {i:2d}. {' '.join(trigram):40s} - {count:5,} times\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize bigrams and trigrams\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Bigrams\n",
        "bigram_labels = [' '.join(bg) for bg, _ in bigrams]\n",
        "bigram_counts = [count for _, count in bigrams]\n",
        "axes[0].barh(range(len(bigram_labels)), bigram_counts, color='teal')\n",
        "axes[0].set_yticks(range(len(bigram_labels)))\n",
        "axes[0].set_yticklabels(bigram_labels, fontsize=9)\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].set_title('Top 15 Bigrams', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Frequency')\n",
        "\n",
        "# Trigrams\n",
        "trigram_labels = [' '.join(tg) for tg, _ in trigrams]\n",
        "trigram_counts = [count for _, count in trigrams]\n",
        "axes[1].barh(range(len(trigram_labels)), trigram_counts, color='coral')\n",
        "axes[1].set_yticks(range(len(trigram_labels)))\n",
        "axes[1].set_yticklabels(trigram_labels, fontsize=9)\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].set_title('Top 15 Trigrams', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bigrams by emotion\n",
        "print(\"\\nüéØ Top 5 Bigrams by Emotion:\\n\")\n",
        "\n",
        "for emotion in emotion_mapping.values():\n",
        "    emotion_texts = df[df['emotion_name'] == emotion]['text']\n",
        "    emotion_bigrams = get_ngrams(emotion_texts, n=2, top_k=5)\n",
        "    \n",
        "    print(f\"\\n{emotion}:\")\n",
        "    for i, (bigram, count) in enumerate(emotion_bigrams, 1):\n",
        "        print(f\"  {i}. {' '.join(bigram):25s} - {count:4,} times\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Text Complexity & Diversity Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate additional complexity metrics\n",
        "print(\"\\nüìä Text Complexity Metrics by Emotion:\\n\")\n",
        "\n",
        "complexity_stats = df.groupby('emotion_name').agg({\n",
        "    'text_length': 'mean',\n",
        "    'word_count': 'mean',\n",
        "    'avg_word_length': 'mean',\n",
        "    'unique_words': 'mean',\n",
        "    'unique_word_ratio': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "display(complexity_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize text complexity metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Average word length by emotion\n",
        "sns.barplot(data=df, x='emotion_name', y='avg_word_length', ax=axes[0, 0], \n",
        "            palette='viridis', estimator=np.mean, ci=None)\n",
        "axes[0, 0].set_title('Average Word Length by Emotion', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Emotion')\n",
        "axes[0, 0].set_ylabel('Average Word Length')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Unique word ratio by emotion\n",
        "sns.boxplot(data=df, x='emotion_name', y='unique_word_ratio', ax=axes[0, 1], palette='coolwarm')\n",
        "axes[0, 1].set_title('Unique Word Ratio Distribution by Emotion', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Emotion')\n",
        "axes[0, 1].set_ylabel('Unique Word Ratio')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Unique words by emotion\n",
        "sns.violinplot(data=df, x='emotion_name', y='unique_words', ax=axes[1, 0], palette='Set2')\n",
        "axes[1, 0].set_title('Unique Words Distribution by Emotion', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Emotion')\n",
        "axes[1, 0].set_ylabel('Number of Unique Words')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Correlation heatmap\n",
        "numeric_cols = ['text_length', 'word_count', 'avg_word_length', 'unique_words', 'unique_word_ratio', 'label']\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 1], \n",
        "            cbar_kws={'label': 'Correlation'})\n",
        "axes[1, 1].set_title('Feature Correlation Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Sample Text Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample texts from each emotion\n",
        "print(\"\\nüìù Sample Texts by Emotion (5 examples per emotion):\\n\")\n",
        "\n",
        "for emotion in emotion_mapping.values():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{emotion.upper()}\")\n",
        "    print('='*80)\n",
        "    \n",
        "    samples = df[df['emotion_name'] == emotion]['text'].sample(n=5, random_state=42)\n",
        "    \n",
        "    for i, text in enumerate(samples, 1):\n",
        "        print(f\"\\n{i}. {text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find longest and shortest texts\n",
        "print(\"\\nüìè Text Length Extremes:\\n\")\n",
        "\n",
        "# Longest texts\n",
        "print(\"\\nüîπ Top 3 Longest Texts:\\n\")\n",
        "longest_texts = df.nlargest(3, 'text_length')[['text', 'emotion_name', 'text_length', 'word_count']]\n",
        "for idx, row in longest_texts.iterrows():\n",
        "    print(f\"Emotion: {row['emotion_name']}\")\n",
        "    print(f\"Length: {row['text_length']} chars, {row['word_count']} words\")\n",
        "    print(f\"Text: {row['text']}\")\n",
        "    print()\n",
        "\n",
        "# Shortest texts\n",
        "print(\"\\nüîπ Top 3 Shortest Texts:\\n\")\n",
        "shortest_texts = df.nsmallest(3, 'text_length')[['text', 'emotion_name', 'text_length', 'word_count']]\n",
        "for idx, row in shortest_texts.iterrows():\n",
        "    print(f\"Emotion: {row['emotion_name']}\")\n",
        "    print(f\"Length: {row['text_length']} chars, {row['word_count']} words\")\n",
        "    print(f\"Text: {row['text']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Stopwords Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze stopwords presence\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def count_stopwords(text):\n",
        "    words = text.lower().split()\n",
        "    return sum(1 for word in words if word in stop_words)\n",
        "\n",
        "df['stopword_count'] = df['text'].apply(count_stopwords)\n",
        "df['stopword_ratio'] = df['stopword_count'] / df['word_count']\n",
        "\n",
        "print(\"\\nüõë Stopword Statistics:\")\n",
        "print(f\"  Average stopwords per text: {df['stopword_count'].mean():.2f}\")\n",
        "print(f\"  Average stopword ratio: {df['stopword_ratio'].mean():.2%}\")\n",
        "\n",
        "print(\"\\nüìä Stopword Statistics by Emotion:\")\n",
        "stopword_stats = df.groupby('emotion_name')[['stopword_count', 'stopword_ratio']].mean().round(3)\n",
        "display(stopword_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize stopword distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Stopword count distribution\n",
        "axes[0].hist(df['stopword_count'], bins=30, color='salmon', edgecolor='black')\n",
        "axes[0].set_title('Stopword Count Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Number of Stopwords')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(df['stopword_count'].mean(), color='red', linestyle='--', \n",
        "                label=f\"Mean: {df['stopword_count'].mean():.1f}\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Stopword ratio by emotion\n",
        "sns.boxplot(data=df, x='emotion_name', y='stopword_ratio', ax=axes[1], palette='pastel')\n",
        "axes[1].set_title('Stopword Ratio by Emotion', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Emotion')\n",
        "axes[1].set_ylabel('Stopword Ratio')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Key Insights & Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä EDA SUMMARY - KEY INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£ DATASET OVERVIEW:\")\n",
        "print(f\"   ‚Ä¢ Total samples: {len(df):,}\")\n",
        "print(f\"   ‚Ä¢ Number of emotions: {df['label'].nunique()}\")\n",
        "print(f\"   ‚Ä¢ Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"   ‚Ä¢ Duplicate rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£ CLASS DISTRIBUTION:\")\n",
        "for emotion, count in class_counts.items():\n",
        "    print(f\"   ‚Ä¢ {emotion:10s}: {count:6,} ({count/len(df)*100:5.2f}%)\")\n",
        "print(f\"   ‚Ä¢ Class imbalance ratio: {max_ratio:.2f}\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£ TEXT STATISTICS:\")\n",
        "print(f\"   ‚Ä¢ Average text length: {df['text_length'].mean():.1f} characters\")\n",
        "print(f\"   ‚Ä¢ Average word count: {df['word_count'].mean():.1f} words\")\n",
        "print(f\"   ‚Ä¢ Average word length: {df['avg_word_length'].mean():.2f} characters\")\n",
        "print(f\"   ‚Ä¢ Text length range: {df['text_length'].min()} - {df['text_length'].max()} characters\")\n",
        "print(f\"   ‚Ä¢ Word count range: {df['word_count'].min()} - {df['word_count'].max()} words\")\n",
        "\n",
        "print(f\"\\n4Ô∏è‚É£ VOCABULARY:\")\n",
        "print(f\"   ‚Ä¢ Total words: {total_words:,}\")\n",
        "print(f\"   ‚Ä¢ Unique words: {unique_words:,}\")\n",
        "print(f\"   ‚Ä¢ Vocabulary richness: {vocab_richness:.4f}\")\n",
        "print(f\"   ‚Ä¢ Average unique word ratio per text: {df['unique_word_ratio'].mean():.3f}\")\n",
        "\n",
        "print(f\"\\n5Ô∏è‚É£ STOPWORDS:\")\n",
        "print(f\"   ‚Ä¢ Average stopwords per text: {df['stopword_count'].mean():.2f}\")\n",
        "print(f\"   ‚Ä¢ Average stopword ratio: {df['stopword_ratio'].mean():.2%}\")\n",
        "\n",
        "print(f\"\\n6Ô∏è‚É£ RECOMMENDATIONS FOR PREPROCESSING:\")\n",
        "print(f\"   ‚úì Remove stopwords (they comprise ~{df['stopword_ratio'].mean():.0%} of text)\")\n",
        "print(f\"   ‚úì Consider handling duplicates ({duplicates} found)\")\n",
        "print(f\"   ‚úì Text length varies significantly - consider padding/truncation\")\n",
        "if max_ratio > 3:\n",
        "    print(f\"   ‚ö† Handle class imbalance (ratio: {max_ratio:.2f}) - consider class weights\")\n",
        "print(f\"   ‚úì Vocabulary size ({unique_words:,}) suggests embeddings dimension 50-300\")\n",
        "\n",
        "print(f\"\\n7Ô∏è‚É£ MODELING RECOMMENDATIONS:\")\n",
        "print(f\"   ‚Ä¢ Suggested max sequence length: {int(df['word_count'].quantile(0.95))} words (95th percentile)\")\n",
        "print(f\"   ‚Ä¢ Embedding dimension: 100-300 (given vocabulary size)\")\n",
        "print(f\"   ‚Ä¢ Consider using pre-trained embeddings (GloVe, Word2Vec)\")\n",
        "print(f\"   ‚Ä¢ Use dropout and regularization (high vocabulary richness)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ EDA COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save extended dataframe with features for future use (optional)\n",
        "print(\"\\nüíæ Saving extended dataframe with calculated features...\")\n",
        "df.to_csv('./data/train_with_features.csv', index=False)\n",
        "print(\"‚úÖ Saved to './data/train_with_features.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
